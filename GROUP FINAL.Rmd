---
title: "STAT 602_FINAL_GROUP 11"
author: "GROUP 11"
date: "4/28/2021"
output:
  html_document: default
code_folding: hide
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```



```{r, include=FALSE}
## Library
library(knitr)
library(dplyr)
library(ggplot2)
library(viridis)
library(hrbrthemes)
library(corrplot)
library(class)
library(caret)
library(rpart)
library(maptree)
#library(pca3d)
library(MASS)
library(tibble)
library(randomForest)
library(e1071)
library(kernlab)
library(kableExtra)
library(gridExtra)

theme_set(theme_light())
```

## Read data & statistics summary

```{r, include=FALSE}
#labeled <- read.csv('C:/course/602-mordern applied statistics 2/hw/final/labeled.csv') %>% #dplyr::select(-X)
#sampA <- read.csv('C:/course/602-mordern applied statistics 2/hw/final/samp.A.csv')%>% #dplyr::select(-X)
#sampB <- read.csv('C:/course/602-mordern applied statistics 2/hw/final/samp.B.csv')%>% #dplyr::select(-X)
#sampC <- read.csv('C:/course/602-mordern applied statistics 2/hw/final/samp.C.csv')%>% #dplyr::select(-X)

labeled <- read.csv('labeled.csv') %>% dplyr::select(-X)%>%mutate(Class=as.factor(Class))
sampA <- read.csv('samp.A.csv')%>% dplyr::select(-X)
sampB <- read.csv('samp.B.csv')%>% dplyr::select(-X)
sampC <- read.csv('samp.C.csv')%>% dplyr::select(-X)

labeled$Class <- as.factor(labeled$Class)
#set up a new variable 'Roundness'
#Roundness = 4*Area*pi/(perimeter)^2 (refer to the dry bean paper)
#Roundess <- 4*pi*labeled$Area/(labeled$Perimeter)^2
labeled$Roundness <- 4*pi*labeled$Area/(labeled$Perimeter)^2
#labeled <- add_column(labeled, Roundness = Roundess, .after = 7)
sampA$Roundness <- 4*pi*sampA$Area/(sampA$Perimeter)^2
sampB$Roundness <- 4*pi*sampB$Area/(sampB$Perimeter)^2
sampC$Roundness <- 4*pi*sampC$Area/(sampC$Perimeter)^2

#convert Class into factor
#labeled$Class <- as.factor(labeled$Class)
#
##set up a new variable 'Roundness'
##Roundness = 4*Area*pi/(perimeter)^2 (refer to the dry bean paper)
#Roundess <- 4*pi*labeled$Area/(labeled$Perimeter)^2
#labeled <- add_column(labeled, Roundness = Roundess, .after = 7)
#sampA$Roundness <- 4*pi*sampA$Area/(sampA$Perimeter)^2
#sampB$Roundness <- 4*pi*sampB$Area/(sampB$Perimeter)^2
#sampC$Roundness <- 4*pi*sampC$Area/(sampC$Perimeter)^2
#
#check for duplicate rows
dup.rows = sum(labeled%>%duplicated(), sampA%>%duplicated(),
               sampB%>%duplicated(),sampC%>%duplicated())
```


## Data exploration for labeled data




```{r, warning=F, message=F, include=FALSE}

summary.stats <- round(as.data.frame((labeled[,-8])%>%psych::describe())%>%dplyr::select(n,mean, sd, median, min, max, range, se), 3)


kable(summary.stats, caption="Statistical distribution of features of dry beans varieties(in pixels)")%>%kable_styling(latex_option=c("hold_position"), full_width = F)%>%kable_classic(html_font = "Cambria")
```


```{r, include=FALSE}
summary.stats.A <- round(as.data.frame((sampA)%>%psych::describe())%>%dplyr::select(n,mean, sd, median, min, max, range), 3)
kable(summary.stats.A, caption="Statistical distribution of features of dry beans varieties(in pixels) - Sample A")%>%kable_styling(latex_option=c("hold_position"), full_width = F)%>%kable_classic(html_font = "Cambria")
```


```{r, include=FALSE}
summary.stats.B <- round(as.data.frame((sampB)%>%psych::describe())%>%dplyr::select(n,mean, sd, median, min, max, range), 3)
kable(summary.stats.B, caption="Statistical distribution of features of dry beans varieties(in pixels) - Sample B")%>%kable_styling(latex_option=c("hold_position"), full_width = F)%>%kable_classic(html_font = "Cambria")
```


```{r}
summary.stats.C <- round(as.data.frame((sampC)%>%psych::describe())%>%dplyr::select(n,mean, sd, median, min, max, range), 3)
kable(summary.stats.C, caption="Statistical distribution of features of dry beans varieties(in pixels) - Sample C")%>%kable_styling(latex_option=c("hold_position"), full_width = F)%>%kable_classic(html_font = "Cambria")
```

```{r}
var.tab1 <- labeled%>%group_by(Class)%>% summarize(Var.Area=sd(Area),
                                       Var.Perimeter=var(Perimeter), var.Maj.Axis.=var(MajorAxisLength), var.Min.Axis.=var(MinorAxisLength), var.Eccentricity=min(Eccentricity))

var.tab2 <- labeled%>%group_by(Class)%>% summarize(var.ConvexArea=max(ConvexArea), var.Extent=max(Extent), var.Roundness=max(Roundness))

kable(var.tab1, caption = "Variance of distribution")%>%kable_styling(latex_option=c("hold_position"), full_width = F)%>%kable_classic(html_font = "Cambria")
```

```{r}
kable(var.tab2, caption = "Variance of distribution")%>%kable_styling(latex_option=c("hold_position"), full_width = F)%>%kable_classic(html_font = "Cambria")
```



```{r}

classes <- c("BOMBAY", "CALI", "DERMASON", "HOROZ", "SEKER", "SIRA")

price.per.1b <- c("$5.56", "$6.02", "$1.98", "$2.43", "$2.72", "$5.40")

price.per.pound <- c(5.56, 6.02, 1.98, 2.43, 2.72, 5.40)
names(price.per.pound) <- classes

grams.per.seed <- c(1.92, 0.61, 0.28, 0.52, 0.49, 0.38)
names(grams.per.seed) <- classes

grams.per.pound <- 453.592

price.per.seed <- round(((price.per.pound*grams.per.seed)/grams.per.pound),6)

price.weight.data <-  cbind(price.per.1b, grams.per.seed, price.per.seed)

kable(price.weight.data, col.names=c("price per pound", "grams per seed", "price per seed"), caption="distribution of types of dry beans and prices per seed")%>%kable_styling(latex_option=c("hold_position"), full_width = F)%>%kable_classic(html_font = "Cambria")
```


### Histogram of each feature - Labeled Data
```{r, fig.height=9, message=F, fig.cap="Histograms of Variables - Labeled Data"}
grid.arrange(
labeled%>%ggplot() + geom_histogram(aes(x=Area), fill="light blue", col="brown")+ labs(title = "Area"),
labeled%>%ggplot() + geom_histogram(aes(x=Perimeter), fill="light blue", col="brown")+ labs(title = "Perimeter"),
labeled%>%ggplot() + geom_histogram(aes(x=MajorAxisLength), fill="light blue", col="brown")+ labs(title = "MajorAxisLength"),
labeled%>%ggplot() + geom_histogram(aes(x=MinorAxisLength), fill="light blue", col="brown")+ labs(title = "MinorAxisLength"),
labeled%>%ggplot() + geom_histogram(aes(x=ConvexArea), fill="light blue", col="brown")+ labs(title = "ConvexArea"),
labeled%>%ggplot() + geom_histogram(aes(x=Roundness), fill="light blue", col="brown")+ labs(title = "Roundness"),
labeled%>%ggplot() + geom_histogram(aes(x=Extent), fill="light blue", col="brown")+ labs(title = "Extent"),ncol=2)
```

### Histogram of each feature -Sample A
```{r, fig.height=9, message=F, fig.cap="Histograms of Variables - Sample A"}
grid.arrange(
sampA%>%ggplot() + geom_histogram(aes(x=Area), fill="light blue", col="brown")+ labs(title = "Area"),
sampA%>%ggplot() + geom_histogram(aes(x=Perimeter), fill="light blue", col="brown")+ labs(title = "Perimeter"),
sampA%>%ggplot() + geom_histogram(aes(x=MajorAxisLength), fill="light blue", col="brown")+ labs(title = "MajorAxisLength"),
sampA%>%ggplot() + geom_histogram(aes(x=MinorAxisLength), fill="light blue", col="brown")+ labs(title = "MinorAxisLength"),
sampA%>%ggplot() + geom_histogram(aes(x=ConvexArea), fill="light blue", col="brown")+ labs(title = "ConvexArea"),
sampA%>%ggplot() + geom_histogram(aes(x=Roundness), fill="light blue", col="brown")+ labs(title = "Roundness"),
sampA%>%ggplot() + geom_histogram(aes(x=Extent), fill="light blue", col="brown")+ labs(title = "Extent"),ncol=2)
```


### Histogram of each feature - Sample B
```{r, fig.height=9, message=F, fig.cap="Histograms of Variables - Sample B"}
grid.arrange(
sampB%>%ggplot() + geom_histogram(aes(x=Area), fill="light blue", col="brown")+ labs(title = "Area"),
sampB%>%ggplot() + geom_histogram(aes(x=Perimeter), fill="light blue", col="brown")+ labs(title = "Perimeter"),
sampB%>%ggplot() + geom_histogram(aes(x=MajorAxisLength), fill="light blue", col="brown")+ labs(title = "MajorAxisLength"),
sampB%>%ggplot() + geom_histogram(aes(x=MinorAxisLength), fill="light blue", col="brown")+ labs(title = "MinorAxisLength"),
sampB%>%ggplot() + geom_histogram(aes(x=ConvexArea), fill="light blue", col="brown")+ labs(title = "ConvexArea"),
sampB%>%ggplot() + geom_histogram(aes(x=Roundness), fill="light blue", col="brown")+ labs(title = "Roundness"),
sampB%>%ggplot() + geom_histogram(aes(x=Extent), fill="light blue", col="brown")+ labs(title = "Extent"),ncol=2)
```


### Histogram of each feature - Sample B
```{r, fig.height=9, message=F, fig.cap="Histograms of Variables - Sample C"}
grid.arrange(
sampC%>%ggplot() + geom_histogram(aes(x=Area), fill="light blue", col="brown")+ labs(title = "Area"),
sampC%>%ggplot() + geom_histogram(aes(x=Perimeter), fill="light blue", col="brown")+ labs(title = "Perimeter"),
sampC%>%ggplot() + geom_histogram(aes(x=MajorAxisLength), fill="light blue", col="brown")+ labs(title = "MajorAxisLength"),
sampC%>%ggplot() + geom_histogram(aes(x=MinorAxisLength), fill="light blue", col="brown")+ labs(title = "MinorAxisLength"),
sampC%>%ggplot() + geom_histogram(aes(x=ConvexArea), fill="light blue", col="brown")+ labs(title = "ConvexArea"),
sampC%>%ggplot() + geom_histogram(aes(x=Roundness), fill="light blue", col="brown")+ labs(title = "Roundness"),
sampC%>%ggplot() + geom_histogram(aes(x=Extent), fill="light blue", col="brown")+ labs(title = "Extent"),ncol=2)
```
  

### Boxplot and Violin plots for each class

```{r, fig.height=11, fig.width=9, fig.cap="Boxplots of Variables by Classes"}
library(gridExtra)
grid.arrange(
labeled%>%group_by(Class)%>%ggplot() + geom_boxplot(aes(x=Class, y=Area), col="brown")  + labs(title = "Boxplot of class vs Area") + geom_violin(aes(x=Class, y=Area), alpha=0.4),
labeled%>%group_by(Class)%>%ggplot() + geom_boxplot(aes(x=Class, y=Perimeter), col="brown") + labs(title = "Boxplot of class vs Perimeter") + geom_violin(aes(x=Class, y=Perimeter), alpha=0.4),
labeled%>%group_by(Class)%>%ggplot() + geom_boxplot(aes(x=Class, y=MajorAxisLength), col="brown") + labs(title = "Boxplot of class vs MajorAxisLength") + geom_violin(aes(x=Class, y=MajorAxisLength), alpha=0.4),
labeled%>%group_by(Class)%>%ggplot() + geom_boxplot(aes(x=Class, y=MinorAxisLength), col="brown") + labs(title = "Boxplot of class vs MinorAxisLength") + geom_violin(aes(x=Class, y=MinorAxisLength), alpha=0.4),
labeled%>%group_by(Class)%>%ggplot() + geom_boxplot(aes(x=Class, y=ConvexArea), col="brown") + labs(title = "Boxplot of class vs ConvexArea") + geom_violin(aes(x=Class, y=ConvexArea), alpha=0.4),
labeled%>%group_by(Class)%>%ggplot() + geom_boxplot(aes(x=Class, y=Roundness), col="brown") + labs(title = "Boxplot of class vs Roundness") + geom_violin(aes(x=Class, y=Roundness), alpha=0.4),
labeled%>%group_by(Class)%>%ggplot() + geom_boxplot(aes(x=Class, y=Extent), col="brown") + labs(title = "Boxplot of class vs Extent") + geom_violin(aes(x=Class, y=Extent), alpha=0.4),
labeled%>%group_by(Class)%>%ggplot() + geom_boxplot(aes(x=Class, y=Eccentricity), col="brown") + labs(title = "Boxplot of class vs Eccentricity") +geom_violin(aes(x=Class, y=Eccentricity), alpha=0.4),ncol=2)
```



### Correlation plot of the labeled dataset
```{r, fig.cap='Correlation plot of the labeled dataset'}
corrplot(cor(labeled%>% dplyr::select(-Class)), method = 'ellipse', type = "lower")
```



### correlation plot of the sample A dataset 

```{r, fig.cap='Correlation plot of the sample A dataset'}
corrplot(cor(sampA), method = 'ellipse', type = "lower")
```


### correlation plot of the sample B dataset 

```{r, fig.cap='Correlation plot of the sample B dataset'}
corrplot(cor(sampB), method = 'ellipse', type = "lower")
```


### correlation plot of the sample C dataset 

```{r, fig.cap='Correlation plot of the sample C dataset'}
corrplot(cor(sampC), method = 'ellipse', type = "lower")
```


### correlation plot by class for labeled dataset 
```{r, fig.width=5, fig.height=4, fig.cap="correlation plot of the by class for labeled dataset",, warning=F}
library(GGally)
#labeled%>%filter(Class=="BOMBAY")%>%ggcorr()


par(mfrow=c(2,3))

test <- labeled%>%filter(Class=="BOMBAY")%>% dplyr::select(-Class)
corrplot(cor(test ), method = 'ellipse', type = "lower")

testb <- labeled%>%filter(Class=="CALI")%>% dplyr::select(-Class)
corrplot(cor(testb), method = 'ellipse', type = "lower")

test <- labeled%>%filter(Class=="DERMASON")%>% dplyr::select(-Class)
corrplot(cor(test), method = 'ellipse', type = "lower")

test <- labeled%>%filter(Class=="SEKER")%>% dplyr::select(-Class)
corrplot(cor(test), method = 'ellipse', type = "lower")

test <- labeled%>%filter(Class=="SIRA")%>% dplyr::select(-Class)
corrplot(cor(test), method = 'ellipse', type = "lower")

test <- labeled%>%filter(Class=="HOROZ")%>% dplyr::select(-Class)
corrplot(cor(test), method = 'ellipse', type = "lower")

```


### Principal Component analysis

```{r}
#############pca#############
pca.labeled <- prcomp(labeled %>% dplyr::select(-Class), scale = TRUE)
pca.sampA <- prcomp(sampA, scale = TRUE)
pca.sampB <- prcomp(sampB, scale = TRUE)
pca.sampC <- prcomp(sampC, scale = TRUE)
```

```{r, fig.cap='Variance explained by each components'}
#plot the variance explained by the first few principal components.
par(mfrow = c(2,2))
plot(pca.labeled, col="blue")
plot(pca.sampA, col="blue")
plot(pca.sampB, col="blue")
plot(pca.sampC, col="blue")
#library(viridisLite)
```

```{r, fig.cap='Commulative Variance explained by each components'}
#plot the variance explained by the first few principal components.
par(mfrow = c(2,2))
plot(cumsum(pca.labeled$sdev^2 / sum(pca.labeled$sdev^2)), 
     xlab = 'PC', ylab = 'Cumm Variance Explained', main = 'pca.labeled', col="blue")
abline(h=0.9, col='red')
plot(cumsum(pca.sampA$sdev^2 / sum(pca.sampA$sdev^2)), 
     xlab = 'PC', ylab = 'Cumm Variance Explained', main = 'pca.sampA', col="blue")
abline(h=0.9, col='red')
plot(cumsum(pca.sampB$sdev^2 / sum(pca.sampB$sdev^2)), 
     xlab = 'PC', ylab = 'Cumm Variance Explained', main = 'pca.sampB', col="blue")
abline(h=0.9, col='red')
plot(cumsum(pca.sampC$sdev^2 / sum(pca.sampC$sdev^2)), 
     xlab = 'PC', ylab = 'Cumm Variance Explained', main = 'pca.sampC', col="blue")
abline(h=0.9, col='red')
```

\newpage

## Model performance function

```{r}
perf.measure <- function(Preds, Truth){
  Preds <- as.character(Preds)
  Truth <- as.character(Truth)
  CV.tab.dat <- cbind(Preds, Truth)
  conf.tab <- xtabs(~Preds+Truth, CV.tab.dat)
  #accuracy rate
  accuracy.rate <- round(mean(Preds==Truth),2)
  #error rate
  error.rate <- round(1-accuracy.rate, 2)
  #each Class
  tp <- c(conf.tab[1,1], conf.tab[2,2], conf.tab[3,3], 
        conf.tab[4,4], conf.tab[5,5], conf.tab[6,6])
  fp <- apply(conf.tab, 1, sum) - tp
  fn <- apply(conf.tab, 2, sum) - tp
  tn <- sum(conf.tab) - tp - fn - fp
  #precision (true positive among all predicted positive)
  precision.Class <- round(tp/(tp+fp),2)
  precision.Avg <- round(mean(tp/(tp+fp)),2)
  #recall (percent of all positives are corrected predicted)
  recall.Class <- round(tp/(tp+fn),2)
  recall.Avg <- round(mean(tp/(tp+fn)),2)
  #specificity (percent of all negatives are corrected predicted)
  specificity.Class <- round(tn/(tn+fp),2)
  specificity.Avg <- round(mean(tn/(tn+fp)),2)
  #F1.score = 2*precision*recall / (precision+recall)
  F1.score.Class <- round((2* tp/(tp+fp)* tp/(tp+fn))/(tp/(tp+fp) + tp/(tp+fn)),2)
  F1.score.Avg <- round(mean((2* tp/(tp+fp)* tp/(tp+fn))/(tp/(tp+fp) + tp/(tp+fn))),2)
  return(list(accuracy.rate = accuracy.rate, error.rate = error.rate, 
              precision.Class = precision.Class, precision.Avg = precision.Avg, 
              recall.Class = recall.Class, recall.Avg = recall.Avg, 
              specificity.Class = specificity.Class, specificity.Avg = specificity.Avg, 
              F1.score.Class = F1.score.Class, F1.score.Avg = F1.score.Avg,
              conf.tab <- conf.tab))
}
```


## Construct labled.sc dataset and pca dataset

```{r}
#construc scaled label data
labeled.sc <- as.data.frame(scale(labeled %>% dplyr::select(-Class)))
labeled.sc$Class <- labeled$Class

#construct pca label data
labeled.pca <- as.data.frame(pca.labeled$x)
labeled.pca$Class <- labeled$Class
```

## Model fitting (LOOCV)

### LDA

```{r}
#fit lda and predict with CV (leave-one-out cross validation)
lda.all <- lda(Class~., data = labeled, CV = TRUE)
lda.3var <- lda(Class ~ Area + Eccentricity + Extent,
                data = labeled, CV = TRUE)
lda.4var <- lda(Class ~ Area + Eccentricity + Extent + Roundness,
                data = labeled, CV = TRUE)
lda.3pca <- lda(Class ~ PC1 + PC2 + PC3, 
                data = labeled.pca, CV = TRUE)

#lda CV performance
lda.all.perf <- perf.measure(Preds = lda.all$class, Truth = labeled$Class)
lda.3var.perf <- perf.measure(Preds = lda.3var$class, Truth = labeled$Class)
lda.4var.perf <- perf.measure(Preds = lda.4var$class, Truth = labeled$Class)
lda.3pca.perf <- perf.measure(Preds = lda.3pca$class, Truth = labeled$Class)
```


### QDA

```{r}
#fit qda and predict with CV (leave-one-out cross validation)
qda.all <- qda(Class~., data = labeled, CV = TRUE)
qda.3var <- qda(Class ~ Area + Eccentricity + Extent,
                data = labeled, CV = TRUE)
qda.4var <- qda(Class ~ Area + Eccentricity + Extent + Roundness,
                data = labeled, CV = TRUE)
qda.3pca <- qda(Class ~ PC1 + PC2 + PC3, 
                data = labeled.pca, CV = TRUE)

#qda CV performance
qda.all.perf <- perf.measure(Preds = qda.all$class, Truth = labeled$Class)
qda.3var.perf <- perf.measure(Preds = qda.3var$class, Truth = labeled$Class)
qda.4var.perf <- perf.measure(Preds = qda.4var$class, Truth = labeled$Class)
qda.3pca.perf <- perf.measure(Preds = qda.3pca$class, Truth = labeled$Class)
```


### randomForest

```{r}
set.seed(12345)
#fit randomForest and predict with CV (leave-one-out cross validation)
forest.all<-randomForest(Class~., data = labeled,CV = TRUE, ntree=500,mtry=2) 
forest.3var<-randomForest(Class~Area + Eccentricity + Extent, data = labeled, CV = TRUE, ntree=500,mtry=1) 
forest.4var<-randomForest(Class~Area + Eccentricity + Extent + Roundness, data = labeled, CV = TRUE, ntree=500,mtry=2) 
forest.3pca<-randomForest(Class~PC1 + PC2 + PC3, data = labeled.pca, CV = TRUE,ntree=500,mtry=2)

#randomForest CV performance
forest.all.perf <- perf.measure(Preds = forest.all$predicted, Truth = labeled$Class)
forest.3var.perf <- perf.measure(Preds = forest.3var$predicted, Truth = labeled$Class)
forest.4var.perf <- perf.measure(Preds = forest.4var$predicted, Truth = labeled$Class)
forest.3pca.perf <- perf.measure(Preds = forest.3pca$predicted, Truth = labeled$Class)
```



```{r}

```




### KNN 

```{r}
set.seed(12345)
AR.all <- NULL
for (k in 1:100) {
test <- knn.cv(labeled.sc[,1:8],cl=labeled$Class, k)
AR.all[k] <- mean(test==labeled.sc$Class)
}
k.all <- which(AR.all==max(AR.all)) # my result is 15/17
knn.all.sc <- knn.cv(labeled.sc[,1:8],
                     cl=labeled.sc$Class, k=k.all[1])
######
set.seed(12345)
AR.3var <- NULL
for (k in 1:100) {
test <- knn.cv(labeled.sc[,c("Area", "Eccentricity", "Extent")],
               cl=labeled.sc$Class, k)
AR.3var[k] <- mean(test==labeled.sc$Class)
}
k.3var=which(AR.3var==max(AR.3var)) # my result is 17
knn.3var.sc <- knn.cv(labeled.sc[,c("Area", "Eccentricity", "Extent")],
                      cl=labeled.sc$Class, k=k.3var[1])
#########
set.seed(12345)
AR.4var <- NULL
for (k in 1:100) {
test <- knn.cv(labeled.sc[,c("Area", "Eccentricity", "Extent", "Roundness")],
               cl=labeled.sc$Class, k)
AR.4var[k] <- mean(test==labeled.sc$Class)
}
k.4var=which(AR.4var==max(AR.4var)) # my result is 15
knn.4var.sc <- knn.cv(labeled.sc[,c("Area", "Eccentricity", "Extent", "Roundness")],
                      cl=labeled.sc$Class, k=k.4var[1])
########
set.seed(12345)
AR.3pca <- NULL
for (k in 1:100) {
test <- knn.cv(labeled.pca[,1:3],cl=labeled.pca$Class, k)
AR.3pca[k] <- mean(test==labeled.pca$Class)
}
k.3pca=which(AR.3pca==max(AR.3pca)) # my result is 18/19
knn.3pca.sc <- knn.cv(labeled.pca[,1:3],cl=labeled.pca$Class, k=k.3pca[1])
```

```{r, fig.cap='optimal k value choices plots for knn model'}
#knn optimual parameters plot
par(mfrow = c(2,2))
plot(AR.all, ylim=c(0.8,0.9), xlab = 'k value', ylab = 'LOOCV accuracy rate', 
     main = 'knn model with all variables')
abline(v=k.all[1], col = 'red')
legend(x=k.all[1], y=0.85, legend = paste('optimal k=',k.all[1]), bty='n')

plot(AR.3var, ylim=c(0.8,0.9), xlab = 'k value', ylab = 'LOOCV accuracy rate', 
     main = 'knn model with 3 variables')
abline(v=k.3var[1], col = 'red')
legend(x=k.3var[1], y=0.85, legend = paste('optimal k=',k.3var[1]), bty='n')

plot(AR.4var, ylim=c(0.8,0.9), xlab = 'k value', ylab = 'LOOCV accuracy rate', 
     main = 'knn model with 4 variables')
abline(v=k.4var[1], col = 'red')
legend(x=k.4var[1], y=0.88, legend = paste('optimal k=',k.4var[1]), bty='n')

plot(AR.3pca, ylim=c(0.8,0.9), xlab = 'k value', ylab = 'LOOCV accuracy rate', 
     main = 'knn model with first three pca variables')
abline(v=k.3pca[1], col = 'red')
legend(x=k.3pca[1], y=0.88, legend = paste('optimal k=',k.3pca[1]), bty='n')
```

```{r}
#knn CV performance
knn.all.sc.perf <- perf.measure(Preds = knn.all.sc, Truth = labeled$Class)
knn.3var.sc.perf <- perf.measure(Preds = knn.3var.sc, Truth = labeled$Class)
knn.4var.sc.perf <- perf.measure(Preds = knn.4var.sc, Truth = labeled$Class)
knn.3pca.perf <- perf.measure(Preds = knn.3pca.sc, Truth = labeled$Class)
```

### SVM

```{r, eval=FALSE}
set.seed(12345)
#Fitting SVM with LOOCV (leave-one-out cross validation)


trctrl <- trainControl(method = "LOOCV", number = 1, savePredictions = 'final',
                       seed = as.list(rep(1,3001)))

set.seed(12345)
svm_Linear.all <- train(Class ~., data = labeled, method = "svmLinear", 
                    preProcess = c("center", "scale"), tuneLength = 10, trControl = trctrl)
svm_Linear.all


svm_Linear.3var <- train(Class~Area + Eccentricity + Extent, data = labeled, 
                         method = "svmLinear", CV=TRUE, 
                         preProcess = c("center", "scale"), tuneLength = 10)
svm_Linear.3var


svm_Linear.4var <- train(Class~Area + Eccentricity + Extent+Roundness, data = labeled, method = "svmLinear", CV=TRUE, preProcess = c("center", "scale"), tuneLength = 10)
svm_Linear.4var


svm_Linear.3pca <- train(Class ~ PC1 + PC2 + PC3, data = labeled.pca,
                         method = "svmLinear", preProcess = c("center", "scale"), CV=TRUE, tuneLength = 10)
svm_Linear.3pca

#SVM_Linear performance
svm_Linear.all.perf <- perf.measure(Preds = svm_Linear.all$class, Truth = labeled$Class)
svm_Linear.3var.perf <- perf.measure(Preds = svm_Linear.3var$class, Truth = labeled$Class)
svm_Linear.4var.perf <- perf.measure(Preds = svm_Linear.4var$class, Truth = labeled$Class)
svm_Linear.3pca.perf <- perf.measure(Preds = svm_Linear.3pca$class, Truth = labeled$Class)



#Non-linear SVM

svm_NLinear <- train(Class ~., method = "svmRadial", data = labeled, kernel='radial',  preProcess = c("center", "scale"), CV=TRUE, Scale=FALSE)

svm_NLinear <- train(Class ~., method = "svmRadial", data = labeled, preProcess = c("center", "scale"), CV=TRUE, Scale=FALSE)
svm_NLinear

#or

svm_NLinear <- train(Class ~., data = labeled, method = "svmradial",  preProcess = c("center", "scale"), CV=TRUE, Scale=FALSE)
svm_NLinear

tuned = tune.svm(Class~., data = labeled, gamma = 10^-2, cost = 10^2,
                 kernel = 'linear', tunecontrol=tune.control(cross=10))
table(tuned$best.model$fitted)
table(predict(tuned$best.model, labeled))

test3 <- svm(Class~., data = labeled, kernel = 'linear', gamma = 10^-2, cost = 10^2)
table(test3$fitted)
table(predict(test3, labeled))
 
test <- svm(Class~., data = labeled, kernel = 'linear', cross = 3000)
table(test$fitted)
table(predict(test, labeled))

test2 <- svm(Class~., data = labeled, kernel = 'linear')
table(test2$fitted)
table(predict(test2, labeled))


svm_linear.all.sc <-  as.factor(NULL)
levels(svm_linear.all.sc) <- levels(labeled$Class)
for(i in 1:3000){
  train <- labeled[-i,]
  test <- labeled[i,]
  mol <- svm(Class ~., data = train, scale = TRUE, kernel = 'linear')
  svm_linear.all.sc[i] <- predict(mol, newdata = test)
}
#test3 is same as tuned
#test2 is closer to LOOCV
#test is same as test2
```

## construc performance table and plot

```{r}
COL.NAME <- c('lda', 'qda', 'RandomForest', 'knn.sc')
Row.NAME <- c('all.var', '3var', '4var', '3pca')

accuracy.rate <- as.data.frame(rbind(c(lda.all.perf$accuracy.rate, qda.all.perf$accuracy.rate, 
                                       forest.all.perf$accuracy.rate,
                                       knn.all.sc.perf$accuracy.rate),
                                     c(lda.3var.perf$accuracy.rate, qda.3var.perf$accuracy.rate,
                                       forest.3var.perf$accuracy.rate,
                                       knn.3var.sc.perf$accuracy.rate),
                                     c(lda.4var.perf$accuracy.rate, qda.4var.perf$accuracy.rate,
                                       forest.4var.perf$accuracy.rate,
                                       knn.4var.sc.perf$accuracy.rate),
                                     c(lda.3pca.perf$accuracy.rate, qda.3pca.perf$accuracy.rate,
                                       forest.3pca.perf$accuracy.rate,
                                       knn.3pca.perf$accuracy.rate)))

colnames(accuracy.rate) <- COL.NAME
rownames(accuracy.rate) <- Row.NAME


#precision (true positive among all predicted positive)
precision.Avg <- as.data.frame(rbind(c(lda.all.perf$precision.Avg, qda.all.perf$precision.Avg, 
                                       forest.all.perf$precision.Avg,
                                       knn.all.sc.perf$precision.Avg),
                                     c(lda.3var.perf$precision.Avg, qda.3var.perf$precision.Avg,
                                       forest.3var.perf$precision.Avg,
                                       knn.3var.sc.perf$precision.Avg),
                                     c(lda.4var.perf$precision.Avg, qda.4var.perf$precision.Avg,
                                       forest.4var.perf$precision.Avg,
                                       knn.4var.sc.perf$precision.Avg),
                                     c(lda.3pca.perf$precision.Avg, qda.3pca.perf$precision.Avg,
                                       forest.3pca.perf$precision.Avg,
                                       knn.3pca.perf$precision.Avg)))

colnames(precision.Avg) <- COL.NAME
rownames(precision.Avg) <- Row.NAME


#recall (percent of all positives are corrected predicted)
recall.Avg <- as.data.frame(rbind(c(lda.all.perf$recall.Avg, qda.all.perf$recall.Avg, 
                                       forest.all.perf$recall.Avg,
                                       knn.all.sc.perf$recall.Avg),
                                     c(lda.3var.perf$recall.Avg, qda.3var.perf$recall.Avg,
                                       forest.3var.perf$recall.Avg,
                                       knn.3var.sc.perf$recall.Avg),
                                     c(lda.4var.perf$recall.Avg, qda.4var.perf$recall.Avg,
                                       forest.4var.perf$recall.Avg,
                                       knn.4var.sc.perf$recall.Avg),
                                     c(lda.3pca.perf$recall.Avg, qda.3pca.perf$recall.Avg,
                                       forest.3pca.perf$recall.Avg,
                                       knn.3pca.perf$recall.Avg)))

colnames(recall.Avg) <- COL.NAME
rownames(recall.Avg) <- Row.NAME


#specificity (percent of all negatives are corrected predicted)
specificity.Avg <- as.data.frame(rbind(c(lda.all.perf$specificity.Avg, qda.all.perf$specificity.Avg, 
                                       forest.all.perf$specificity.Avg,
                                       knn.all.sc.perf$specificity.Avg),
                                     c(lda.3var.perf$specificity.Avg, qda.3var.perf$specificity.Avg,
                                       forest.3var.perf$specificity.Avg,
                                       knn.3var.sc.perf$specificity.Avg),
                                     c(lda.4var.perf$specificity.Avg, qda.4var.perf$specificity.Avg,
                                       forest.4var.perf$specificity.Avg,
                                       knn.4var.sc.perf$specificity.Avg),
                                     c(lda.3pca.perf$specificity.Avg, qda.3pca.perf$specificity.Avg,
                                       forest.3pca.perf$specificity.Avg,
                                       knn.3pca.perf$specificity.Avg)))
colnames(specificity.Avg) <- COL.NAME
rownames(specificity.Avg) <- Row.NAME


#F1.score = 2*precision*recall / (precision+recall)
F1.score.Avg <- as.data.frame(rbind(c(lda.all.perf$F1.score.Avg, qda.all.perf$F1.score.Avg, 
                                       forest.all.perf$F1.score.Avg,
                                       knn.all.sc.perf$F1.score.Avg),
                                     c(lda.3var.perf$F1.score.Avg, qda.3var.perf$F1.score.Avg,
                                       forest.3var.perf$F1.score.Avg,
                                       knn.3var.sc.perf$F1.score.Avg),
                                     c(lda.4var.perf$F1.score.Avg, qda.4var.perf$F1.score.Avg,
                                       forest.4var.perf$F1.score.Avg,
                                       knn.4var.sc.perf$F1.score.Avg),
                                     c(lda.3pca.perf$F1.score.Avg, qda.3pca.perf$F1.score.Avg,
                                       forest.3pca.perf$F1.score.Avg,
                                       knn.3pca.perf$F1.score.Avg)))
colnames(F1.score.Avg) <- COL.NAME
rownames(F1.score.Avg) <- Row.NAME
```


```{r}
#performance summary table
kable(accuracy.rate, caption = 'Average LOOCV Accuracy Rate across Classes ')
kable(precision.Avg, caption = 'Average LOOCV Precision across Classes ')
kable(recall.Avg, caption = 'Average LOOCV Recall across Classes ')
kable(specificity.Avg, caption = 'Average LOOCV Specificity across Classes ')
kable(F1.score.Avg, caption = 'Average LOOCV F1.score across Classes ')
```


```{r, fig.cap='LOOCV Model Performance Comparision'}
#performance plot
#ar(mfrow=c(1,2))
#lot(x=c(1,2,3,4), y=accuracy.rate[,1], col = rainbow(4)[1], ylim = c(0.8, 1),
#    xlab = 'variable selection', ylab = 'LOOCV accuracy rate',
#    main = 'LOOCV Accuracy Rate versus Models', pch = 1)
#ines(x=c(1,2,3,4), y=accuracy.rate[,1], col = rainbow(4)[1])

#oints(x=c(1,2,3,4), y=accuracy.rate[,2], col = rainbow(4)[2], pch = 2)
#ines(x=c(1,2,3,4), y=accuracy.rate[,2], col = rainbow(4)[2])

#oints(x=c(1,2,3,4), y=accuracy.rate[,3], col = rainbow(4)[3], pch = 3)
#ines(x=c(1,2,3,4), y=accuracy.rate[,3], col = rainbow(4)[3])

#oints(x=c(1,2,3,4), y=accuracy.rate[,4], col = rainbow(4)[4], pch = 4)
#ines(x=c(1,2,3,4), y=accuracy.rate[,4], col = rainbow(4)[4])

#egend('topleft', legend = c('lda','qda','rf','knn'), col = rainbow(4), 
#      pch = 1:4, lty = rep(1,4), bty = 'n')


#plot(x=c(1,2,3,4), y=precision.Avg[,1], col = rainbow(4)[1], ylim = c(0.8, 1),
#     xlab = 'variable selection', ylab = 'LOOCV precision',
#     main = 'LOOCV Precision versus Models', pch = 1)
#lines(x=c(1,2,3,4), y=precision.Avg[,1], col = rainbow(4)[1])
#
#points(x=c(1,2,3,4), y=precision.Avg[,2], col = rainbow(4)[2], pch = 2)
#lines(x=c(1,2,3,4), y=precision.Avg[,2], col = rainbow(4)[2])
#
#points(x=c(1,2,3,4), y=precision.Avg[,3], col = rainbow(4)[3], pch = 3)
#lines(x=c(1,2,3,4), y=precision.Avg[,3], col = rainbow(4)[3])
#
#points(x=c(1,2,3,4), y=precision.Avg[,4], col = rainbow(4)[4], pch = 4)
#lines(x=c(1,2,3,4), y=precision.Avg[,4], col = rainbow(4)[4])
#
#legend('topleft', legend = c('lda','qda','rf','knn'), col = rainbow(4), 
#       pch = 1:4, lty = rep(1,4), bty = 'n')
```

```{r}
library(reshape)
library(gridExtra)
theme_set(theme_light())
```

```{r, fig.width=5}

accuracy.rate$numb.var <- as.factor(c("all.var", "3var", "4var", "3pca"))
mdata <- melt(accuracy.rate, id="numb.var")%>%dplyr::rename(Model="variable")


a <- ggplot(mdata, aes(x=numb.var, y=value, group=Model)) +
  geom_line(aes(color=Model)) +
  geom_point(aes(color=Model)) +
  coord_cartesian(xlim = NULL, ylim = c(0.8,0.95), 
                  expand = TRUE, default = FALSE,clip = "on") + 
  theme(legend.position="top") + labs(title = "Accuracy rate") + 
  xlab("Variable Selection") + ylab("LOOCV Accuracy rate") + 
  guides(fill=guide_legend(title="Model"))

#precision.Avg

precision.Avg$number.var <- as.factor(c("all.var", "3var", "4var", "3pca"))

prec.data <- melt(precision.Avg, id="number.var")%>%dplyr::rename(Model="variable")


b <- ggplot(prec.data, aes(x=number.var, y=value, group=Model)) +
  geom_line(aes(color=Model))+
  geom_point(aes(color=Model)) + coord_cartesian(xlim = NULL, ylim = c(0.8,0.95), 
                  expand = TRUE, default = FALSE,clip = "on") + 
  theme(legend.position="top") + labs(title = "Precision rate") + 
  xlab("Variable Selection") + ylab("LOOCV Precisioin rate") + 
  guides(fill=guide_legend(title="Model"))


#Recall

recall.Avg$number.var <- as.factor(c("all.var", "3var", "4var", "3pca"))

rec.data <- melt(recall.Avg, id="number.var")%>%dplyr::rename(Model="variable")


c <- ggplot(rec.data, aes(x=number.var, y=value, group=Model)) +
  geom_line(aes(color=Model))+
  geom_point(aes(color=Model)) + coord_cartesian(xlim = NULL, ylim = c(0.8,0.95), 
                  expand = TRUE, default = FALSE,clip = "on") + 
  theme(legend.position="top") + labs(title = "Recall rate") + 
  xlab("Variable Selection") + ylab("LOOCV Recall rate") + 
  guides(fill=guide_legend(title="Model"))

#Specificity

specificity.Avg$number.var <- as.factor(c("all.var", "3var", "4var", "3pca"))

spec.data <- melt(specificity.Avg, id="number.var")%>%dplyr::rename(Model="variable")


d <- ggplot(spec.data, aes(x=number.var, y=value, group=Model)) +
  geom_line(aes(color=Model))+
  geom_point(aes(color=Model)) + coord_cartesian(xlim = NULL, ylim = c(0.95, 1), 
                  expand = TRUE, default = FALSE,clip = "on") + 
  theme(legend.position="top") + labs(title = "Specificity rate") + 
  xlab("Variable Selection") + ylab("LOOCV Specificity rate") + 
  guides(fill=guide_legend(title="Model"))

#F1 scoore

F1.score.Avg$number.var <- as.factor(c("all.var", "3var", "4var", "3pca"))

f.data <- melt(F1.score.Avg, id="number.var")%>%dplyr::rename(Model="variable")


e <- ggplot(f.data, aes(x=number.var, y=value, group=Model)) +
  geom_line(aes(color=Model))+
  geom_point(aes(color=Model)) + coord_cartesian(xlim = NULL, ylim = c(0.8, 0.95), 
                  expand = TRUE, default = FALSE,clip = "on") + 
  theme(legend.position="top") + labs(title = "F1 Score") + 
  xlab("Variable Selection") + ylab("LOOCV F1 Score rate") + 
  guides(fill=guide_legend(title="Model"))

```


```{r, fig.height=5, fig.width=5, fig.cap="Model Performance"}

grid.arrange(a,b,c,d,e,ncol=2)
```


## Visualize best-selected model:qda

```{r}
par(mfrow=c(1,2))
color <- c("black", "#DF536B", "#61D04F", "#2297E6", "#28E2E5","#CD0BBC")
palette(color)
plot(labeled.pca[,1:2], col = labeled$Class, main = 'True Classes', 
     xlim = c(-10,10), ylim = c(-5,5))
legend('topright', pch = rep(1,6), col = color, legend = levels(labeled$Class))
plot(labeled.pca[,1:2], col = qda.all$class, main = 'LOOCV qda Predicted Classes', 
     xlim = c(-10,10), ylim = c(-5,5))
```


## Prediction

```{r}
#refit best-selected model
qda.mol <- qda(Class~., data = labeled, CV = FALSE)
#prediction
pred.A <- predict(qda.mol, newdata=sampA)
pred.B <- predict(qda.mol, newdata=sampB)
pred.C <- predict(qda.mol, newdata=sampC)
```

### Visualize prediction

```{r}
#par(mfrow=c(1,2))
#palette(color)
#
#plot(labeled.pca[,1:2], col = labeled$Class, main = 'Label True Classes', 
#     xlim = c(-10,10), ylim = c(-5,5))
#legend('topright', pch = rep(1,6), col = color, legend = levels(labeled$Class))
#
#plot(pca.sampA$x[,1:2], col = pred.A$class, main = 'samp.A predicted Classes',
#     xlim = c(-10,10), ylim = c(-5,5))
#
#
##legend('topright', pch = rep(1,6), col = color, legend = levels(pred.A$class))
#
#plot(pca.sampB$x[,1:2], col = pred.B$class, main = 'samp.B predicted Classes',
#     xlim = c(-10,10), ylim = c(-5,5))
##legend('topright', pch = rep(1,6), col = color, legend = levels(pred.B$class))
#
#plot(pca.sampC$x[,1:2], col = pred.C$class, main = 'samp.C predicted Classes',
#     xlim = c(-10,10), ylim = c(-5,5))
##legend('topright', pch = rep(1,6), col = color, legend = levels(pred.C$class))
#
#
#
pca.sampA.dat <- as.data.frame(cbind(pca.sampA$x, "class"=pred.A$class))%>%mutate(class=as.factor(ifelse(class=="1", "BOMBAY", ifelse(class=="2","CALI", ifelse(class=="3", "DERMASON", ifelse(class=="4", "HOROZ", ifelse(class=="5","SEKER", "SIRA" )))))))
#
#
pca.sampB.dat <- as.data.frame(cbind(pca.sampB$x, "class"=pred.B$class))%>%mutate(class=as.factor(ifelse(class=="1", "BOMBAY", ifelse(class=="2","CALI", ifelse(class=="3", "DERMASON", ifelse(class=="4", "HOROZ", ifelse(class=="5","SEKER", "SIRA" )))))))

pca.sampC.dat <- as.data.frame(cbind(pca.sampC$x, "class"=pred.C$class))%>%mutate(class=as.factor(ifelse(class=="1", "BOMBAY", ifelse(class=="2","CALI", ifelse(class=="3", "DERMASON", ifelse(class=="4", "HOROZ", ifelse(class=="5","SEKER", "SIRA" )))))))
```


```{r, fig.height=5, fig.width=5}
grid.arrange(
ggplot(labeled.pca)+geom_point(aes(x=PC1, y=PC2,col=Class))+ labs(title = "Labeled")+ coord_cartesian(xlim = c(-6,9), ylim = c(-4,3.5), expand = TRUE, default = FALSE,clip = "on")+ 
  theme(legend.position="bottom"),
ggplot(pca.sampA.dat)+geom_point(aes(x=PC1, y=PC2,col=class))+ labs(title = "Sample A") + coord_cartesian(xlim = c(-6,9), ylim = c(-4,3.5), expand = TRUE, default = FALSE,clip = "on")+ 
  theme(legend.position="bottom"),
ggplot(pca.sampB.dat)+geom_point(aes(x=PC1, y=PC2,col=class))+ labs(title = "Sample B") + coord_cartesian(xlim = c(-6,9), ylim = c(-4,3.5), expand = TRUE, default = FALSE,clip = "on")+ 
  theme(legend.position="bottom"),
ggplot(pca.sampC.dat)+geom_point(aes(x=PC1, y=PC2,col=class))+ labs(title = "Sample C") + coord_cartesian(xlim = c(-6,9), ylim = c(-4,3.5), expand = TRUE, default = FALSE,clip = "on")+ 
  theme(legend.position="bottom"),ncol=2)
```


## Prediction Accuracy 


```{r}
Class.name <- c('Bombay', 'Cali', 'Dermason', 'Horoz', 'Seker', 'Sira')
price.per.lbs <- c(5.56, 6.02, 1.98, 2.43, 2.72, 5.40)
names(price.per.lbs) <- Class.name

grams.per.seed <- c(1.92, 0.61, 0.28, 0.52, 0.49, 0.38)
names(grams.per.seed) <- Class.name

grams.per.pound <- 453.592
  
price.per.seed <- price.per.lbs / grams.per.pound * grams.per.seed

kable(as.data.frame(price.per.seed), caption = 'price.per.seed')
```

```{r}
#second method
CV.tab.dat=cbind(Preds=qda.all$class, 
                 Truth=labeled$Class)

conf.tab=xtabs(~Preds+Truth, CV.tab.dat)

#matrix(rowSums(conf.tab), nrow = 6, ncol = 6)

pred.tab.A=conf.probs=conf.tab/
             matrix(rowSums(conf.tab), nrow = 6, ncol = 6)

size.tab <- table(pred.A$class)

pred.accuracy <- function(pred.tab.A, size.tab){
condit.Par.BS=NULL
for (i in 1:1000){
  p.lbs <- NULL
  for (j in 1:6) {
    seed <- t(rmultinom(1, size = size.tab[j], prob = pred.tab.A[j,]))
    p.lbs <- c(p.lbs, seed %*% price.per.seed)
  }
condit.Par.BS=c(condit.Par.BS, sum(p.lbs))
}
return(condit.Par.BS)
}

pred.ar.A <- pred.accuracy(pred.tab.A = pred.tab.A, size.tab = table(pred.A$class))
pred.ar.B <- pred.accuracy(pred.tab.A = pred.tab.A, size.tab = table(pred.B$class))
pred.ar.C <- pred.accuracy(pred.tab.A = pred.tab.A, size.tab = table(pred.C$class))


pred.ar.dat <- rbind(quantile(pred.ar.A, c(0.025, 0.975)),
                     quantile(pred.ar.B, c(0.025, 0.975)),
                     quantile(pred.ar.C, c(0.025, 0.975)))
rownames(pred.ar.dat) <- c('samp.A', 'samp.B', 'samp.C')

kable(pred.ar.dat, caption = 'prediction accuracy')
```









